# ðŸ§… ESXi-Security-Onion-Passive Collection Platform ðŸ§…

## Introduction

This showcases a project similar to what I have built and maintained in my professional work as a Junior Security Engineer while in the military. This cluster is meant to be used on a network as an additional security analysis tool for cyber threat hunting. 

This platform is based off ESXi hypervisors using several virtual machines of a Distributed Build of Security Onion version 2.3.100. 

A Security Onion Distributed Deployment contains the following:
- 1 Master/Manager Node
  - For a central management server in the distributed setup. Responsible for controlling activities of all other nodes in the deployment. 
- 1 Sensor/Forward Node (minimum)
  -  For capturing and processing network traffic and security events with Zeek/Suricata. 
- 1 Search Node (minimum)
  -  For storing and indexing the network logs generated by the Sensor Nodes.

Here is a picture showing the minimum build of a Security Onion Distributed Deployment from the [Security Onion Documentation Website](https://docs.securityonion.net/en/latest/architecture.html):

![image](https://github.com/gervguerrero/ESXi-Security-Onion-Passive-Collection-Platform-/assets/140366635/5cbc6295-e12d-4522-8653-ef46a7b6b2bd)

The highlight of a Distributed Builds allows for greater scalability and performance when adapting to different customer network sizes. Larger networks require more nodes to process a larger volume information, while smaller networks require less. Scaling up and down for Sensor/Forward and Search Nodes is as easy as plug and play to introduce your new asset to the Security Onion cluster, as long as they are built with the same Security Onion version. 

## Customized ESXi Security Onion Build 

![data collection platform purple](https://github.com/gervguerrero/ESXi-Security-Onion-Passive-Collection-Platform-/assets/140366635/fa6ad362-b50f-45a1-acc7-b10ea8f31acf)

Shown above, we can see an overview of the Security Onion cluster collecting raw network traffic from a client's network via SPAN/Mirror ports. 

There are 3 Client Network switches each with a SPAN/Mirror port to show different areas of the network. Each Client switch feeds directly to an ingest monitoring port for a Sensor Node (Light Server). The data is then forwarded to the Manager and Search nodes for an enrichment process running through the Elasticsearch Stack (ELK).  

The Heavy server with ESXi houses a Manager Node and 3 Search nodes where the data after some parsing ultimately sits. The end user Security Analyst on their workstation runs queries in the Kibana GUI web page to pull data out of those processed Elasticsearch indexes, to return data in a visual format via tables, graphs, or other tools availible through the Kibana GUI. 

## Data Flow

In a simple explanation, the Sensor Nodes analyze the network traffic coming off the Switch SPAN/Mirror port with Zeek, Suricata, and Strelka, then sends it through the Elasticsearch stack ELK via Filebeat. It first passes through Logstash and Elasticsearch instances on the Manager Node, and then the Search Node's Logstash and finally stored in the Searh Node's Elasticsearch

![data collection platform purple 2](https://github.com/gervguerrero/ESXi-Security-Onion-Passive-Collection-Platform/assets/140366635/2da5de2f-8153-4d3d-a3f2-0f40132eb52f)

**For a more detailed explanation:**

Once the traffic from the client network (from either SPAN/Mirror Port or In-Line TAP) is ingested into the Sensor Node first, a Berkley Packet Filter (BPF) can be applied that works with Suricata on the NIC level to filter unwanted traffic collection.

Working with the NIC, the data stream reaches **AF_Packet**. AF_Packet is a Linux Kernel tool that allows direct packet capture from the NIC bypassing standard socket mechanisms. Security Onion uses it for high speed and low latency packet capture and creates 4 seperate data streams for each service being used:

- **Zeek**: A metadata protocol analyzer and catergorizer 
- **Suricata**: IDS/IPS tool with protocol/metadata analyzing features 
- **Strelka**: File analysis framework for detecting malware using Yara rules 
- **Steno**: Captures and stores the PCAP, working with Sensoroni to deliver the PCAP for queries in the S.O. Console

**Sensoroni** is just the background service that structures the Security Onion (S.O.) Console web page, and also returns some data on the status of each Security Onion node to display in Grid and Grafana. It also works between the Sensor Node and Manager Node to return a specified timeline of PCAP that a Network Analyst queries in the S.O. Console. 

Once each of the 4 services does their job with the data stream and their specific logs are created, **Filebeat** (a log shipper) sends those logs to the Manager Node to be transformed and mutated by the Manager Node's **Logstash**. 

It will then pass through the Manager Node's Elasticsearch, and then gets redistributed by **Redis** to the Search Node's Logstash. It will then ultimately be indexed and stored by Elasticsearch in the Search Node as a database. 

Now that the has been parsed, enriched, and stored in Elasticsearch in the Search Node, a Security Analyst using **Kibana** on their workstation will query that data. The Search Nodes will search their Elasticsearch database and answer the query delivering the information to Kibana. Kibana will display the information with visuals defined by the analyst, allowing for flexibl stream lined threat hunting.

The **Curator** service on the search node is just to refine and manage elasticsearch indexes.

Our services in each node is color coded based off experience with managing services in the past.




